{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import argparse\n",
    "import time\n",
    "import sys\n",
    "import pickle as pkl\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from src.cnn_model import CharacterLevelCNN\n",
    "from src.data_loader import MyDataset\n",
    "from src import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, training_generator, optimizer, criterion, epoch, writer, print_every=25):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    accuraries = []\n",
    "    num_iter_per_epoch = len(training_generator)\n",
    "\n",
    "    progress_bar = tqdm(enumerate(training_generator),\n",
    "                        total=num_iter_per_epoch)\n",
    "\n",
    "    for iter, batch in progress_bar:\n",
    "        features, labels = batch\n",
    "        if torch.cuda.is_available():\n",
    "            features = features.cuda()\n",
    "            labels = labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(features)\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_metrics = utils.get_evaluation(labels.cpu().numpy(),\n",
    "                                                predictions.cpu().detach().numpy(),\n",
    "                                                list_metrics=[\"accuracy\", \"f1\"])\n",
    "        losses.append(loss.item())\n",
    "        accuraries.append(training_metrics[\"accuracy\"])\n",
    "        f1 = training_metrics['f1']\n",
    "\n",
    "        writer.add_scalar('Train/Loss', loss.item(),\n",
    "                          epoch * num_iter_per_epoch + iter)\n",
    "        writer.add_scalar(\n",
    "            'Train/Accuracy', training_metrics['accuracy'], epoch * num_iter_per_epoch + iter)\n",
    "        writer.add_scalar('Train/f1', f1,\n",
    "                          epoch * num_iter_per_epoch + iter)\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print(\"[Training - Epoch: {}] , Iteration: {}/{} , Loss: {}, Accuracy: {}\".format(\n",
    "                epoch + 1,\n",
    "                iter + 1,\n",
    "                num_iter_per_epoch,\n",
    "                np.mean(losses),\n",
    "                np.mean(accuraries)\n",
    "            ))\n",
    "\n",
    "    return np.mean(losses), np.mean(accuraries)\n",
    "\n",
    "\n",
    "def evaluate(model, validation_generator, criterion, epoch, writer, print_every=25):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    accuraries = []\n",
    "    num_iter_per_epoch = len(validation_generator)\n",
    "\n",
    "    for iter, batch in tqdm(enumerate(validation_generator), total=num_iter_per_epoch):\n",
    "        features, labels = batch\n",
    "        if torch.cuda.is_available():\n",
    "            features = features.cuda()\n",
    "            labels = labels.cuda()\n",
    "        with torch.no_grad():\n",
    "            predictions = model(features)\n",
    "        loss = criterion(predictions, labels)\n",
    "        validation_metrics = utils.get_evaluation(labels.cpu().numpy(),\n",
    "                                                  predictions.cpu().detach().numpy(),\n",
    "                                                  list_metrics=[\"accuracy\", \"f1\"])\n",
    "        accuracy = validation_metrics['accuracy']\n",
    "        f1 = validation_metrics['f1']\n",
    "        losses.append(loss.item())\n",
    "        accuraries.append(accuracy)\n",
    "\n",
    "        writer.add_scalar('Test/Loss', loss.item(),\n",
    "                          epoch * num_iter_per_epoch + iter)\n",
    "        writer.add_scalar('Test/Accuracy', accuracy,\n",
    "                          epoch * num_iter_per_epoch + iter)\n",
    "        writer.add_scalar('Test/f1', f1,\n",
    "                          epoch * num_iter_per_epoch + iter)\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print(\"[Validation - Epoch: {}] , Iteration: {}/{} , Loss: {}, Accuracy: {}\".format(\n",
    "                epoch + 1,\n",
    "                iter + 1,\n",
    "                num_iter_per_epoch,\n",
    "                np.mean(losses),\n",
    "                np.mean(accuraries)))\n",
    "\n",
    "    return np.mean(losses), np.mean(accuraries)\n",
    "\n",
    "\n",
    "def run(args, both_cases=False):\n",
    "\n",
    "    log_path = args.log_path\n",
    "    if os.path.isdir(log_path):\n",
    "        shutil.rmtree(log_path)\n",
    "    os.makedirs(log_path)\n",
    "\n",
    "    if not os.path.exists(args.output):\n",
    "        os.makedirs(args.output)\n",
    "\n",
    "    writer = SummaryWriter(log_path)\n",
    "\n",
    "    batch_size = args.batch_size\n",
    "\n",
    "    training_params = {\"batch_size\": batch_size,\n",
    "                       \"shuffle\": True,\n",
    "                       \"num_workers\": args.workers}\n",
    "\n",
    "    validation_params = {\"batch_size\": batch_size,\n",
    "                         \"shuffle\": False,\n",
    "                         \"num_workers\": args.workers}\n",
    "\n",
    "    full_dataset = MyDataset(args)\n",
    "    train_size = int(args.validation_split * len(full_dataset))\n",
    "    validation_size = len(full_dataset) - train_size\n",
    "    training_set, validation_set = torch.utils.data.random_split(\n",
    "        full_dataset, [train_size, validation_size])\n",
    "    training_generator = DataLoader(training_set, **training_params)\n",
    "    validation_generator = DataLoader(validation_set, **validation_params)\n",
    "\n",
    "    model = CharacterLevelCNN(args)\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if args.optimizer == 'sgd':\n",
    "        optimizer = torch.optim.SGD(\n",
    "            model.parameters(), lr=args.learning_rate, momentum=0.9\n",
    "        )\n",
    "    elif args.optimizer == 'adam':\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(), lr=args.learning_rate\n",
    "        )\n",
    "\n",
    "    best_loss = 1e10\n",
    "    best_epoch = 0\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        training_loss, training_accuracy = train(model,\n",
    "                                                 training_generator,\n",
    "                                                 optimizer,\n",
    "                                                 criterion,\n",
    "                                                 epoch,\n",
    "                                                 writer)\n",
    "\n",
    "        validation_loss, validation_accuracy = evaluate(model,\n",
    "                                                        validation_generator,\n",
    "                                                        criterion,\n",
    "                                                        epoch,\n",
    "                                                        writer)\n",
    "\n",
    "        print('[Epoch: {} / {}]\\ttrain_loss: {:.4f} \\ttrain_acc: {:.4f} \\tval_loss: {:.4f} \\tval_acc: {:.4f}'.\n",
    "              format(epoch + 1, args.epochs, training_loss, training_accuracy, validation_loss, validation_accuracy))\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # learning rate scheduling\n",
    "\n",
    "        if args.schedule != 0:\n",
    "            if args.optimizer == 'sgd' and epoch % args.schedule == 0 and epoch > 0:\n",
    "                current_lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "                current_lr /= 2\n",
    "                print('Decreasing learning rate to {0}'.format(current_lr))\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = current_lr\n",
    "\n",
    "        # early stopping\n",
    "        if validation_loss < best_loss:\n",
    "            best_loss = validation_loss\n",
    "            best_epoch = epoch\n",
    "            if args.checkpoint == 1:\n",
    "                torch.save(model, args.output + 'char_cnn_epoch_{}_{}_{}_loss_{}_acc_{}.pth'.format(args.model_name,\n",
    "                                                                                                    epoch,\n",
    "                                                                                                    optimizer.state_dict()[\n",
    "                                                                                                        'param_groups'][0]['lr'],\n",
    "                                                                                                    round(\n",
    "                                                                                                        validation_loss, 4),\n",
    "                                                                                                    round(\n",
    "                                                                                                        validation_accuracy, 4)\n",
    "                                                                                                    ))\n",
    "\n",
    "        if epoch - best_epoch > args.patience > 0:\n",
    "            print(\"Stop training at epoch {}. The lowest loss achieved is {} at epoch {}\".format(\n",
    "                epoch, validation_loss, best_epoch))\n",
    "            break\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        'Character Based CNN for text classification')\n",
    "    parser.add_argument('--data_path', type=str, default='./data/acllmdb/fulldb.csv')\n",
    "    parser.add_argument('--validation_split', type=float, default=0.8)\n",
    "    parser.add_argument('--label_column', type=str, default='Sentiment')\n",
    "    parser.add_argument('--text_column', type=str, default='SentimentText')\n",
    "    parser.add_argument('--max_rows', type=int, default=50000)\n",
    "    parser.add_argument('--chunksize', type=int, default=25000)\n",
    "    parser.add_argument('--encoding', type=str, default='utf-8')\n",
    "    parser.add_argument('--steps', nargs='+', default=['remove_urls','remove_html','remove_punctuation','lower'])\n",
    "\n",
    "    parser.add_argument('--alphabet', type=str,\n",
    "                        default=\"\"\"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\"\")\n",
    "    parser.add_argument('--number_of_characters', type=int, default=140)\n",
    "    parser.add_argument('--extra_characters', type=str, default='')\n",
    "\n",
    "    parser.add_argument('--config_path', type=str, default='./config.json')\n",
    "    parser.add_argument('--size', type=str,\n",
    "                        choices=['small', 'large'], default='small')\n",
    "\n",
    "    parser.add_argument('--max_length', type=int, default=150)\n",
    "    parser.add_argument('--number_of_classes', type=int, default=2)\n",
    "    parser.add_argument('--epochs', type=int, default=10)\n",
    "    parser.add_argument('--batch_size', type=int, default=128)\n",
    "    parser.add_argument('--optimizer', type=str,\n",
    "                        choices=['adam', 'sgd'], default='sgd')\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.01)\n",
    "    parser.add_argument('--schedule', type=int, default=3)\n",
    "    parser.add_argument('--patience', type=int, default=3)\n",
    "    parser.add_argument('--checkpoint', type=int, choices=[0, 1], default=1)\n",
    "    parser.add_argument('--workers', type=int, default=1)\n",
    "    parser.add_argument('--log_path', type=str, default='./logs/')\n",
    "    parser.add_argument('--output', type=str, default='./models/')\n",
    "    parser.add_argument('--model_name', type=str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_interactive():\n",
    "    return not hasattr(sys.modules['__main__'], '__file__')\n",
    "\n",
    "# work-around for Jupyter notebook and IPython console\n",
    "argv = [] if is_interactive() else sys.argv[1:]\n",
    "argv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "args = parser.parse_args(argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:52, 26.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded successfully with 49582 rows\n"
     ]
    }
   ],
   "source": [
    "full_dataset = MyDataset(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension : 512\n"
     ]
    }
   ],
   "source": [
    "model = CharacterLevelCNN(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
