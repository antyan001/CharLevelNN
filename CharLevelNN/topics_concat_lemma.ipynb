{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import csv\n",
    "import argparse\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "sys.path.insert(0,'../pyutils/')\n",
    "sys.path.insert(0, '/home/ektov-av/python35-libs/lib/python3.5/site-packages/') \n",
    "\n",
    "from pymystem3 import Mystem\n",
    "from corpora_process.utils import extract_subsentences, extend,\\\n",
    "                                  normalize_text, preprocessing_setps, margin_sentences\n",
    "import tqdm    \n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract information about clubid from which discussion boards has been scrapped "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnt=0\n",
    "folders=[]\n",
    "clubsid=[]\n",
    "folder = Path.joinpath(Path(os.getcwd()), 'csv', 'topics')\n",
    "for r, d, f in os.walk(folder):\n",
    "    for folder in d:\n",
    "        folders.append(os.path.join(r, folder))\n",
    "for dir_path in folders:\n",
    "    for the_file in os.listdir(dir_path):\n",
    "        file_path = os.path.join(dir_path, the_file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) and (the_file.startswith('vk_group_topic')):\n",
    "                clubsid.append('club'+the_file.split('vk_group_topic-')[-1].split('_')[0])\n",
    "        except Exception as e:\n",
    "            print(e) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set(clubsid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting a dataframe for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct a dataframe for appropriate category of posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folders=[]\n",
    "folder = Path.joinpath(Path(os.getcwd()), 'csv', 'topics')\n",
    "for r, d, f in os.walk(folder):\n",
    "    for folder in d:\n",
    "        folders.append(os.path.join(r, folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnt_l = 0\n",
    "cnt_t = 0\n",
    "for dir_path in folders:\n",
    "    findcat_tenant   = re.findall(r'.*подсел.*|.*ищу.*|.*сниму.*|.*поиск.*', dir_path)\n",
    "    findcat_landlord = re.findall(r'.*сда.*|.*снят.*|.*посуточно.*', dir_path)\n",
    "    if len(findcat_tenant)!=0:\n",
    "        for the_file in os.listdir(dir_path):\n",
    "            file_path = os.path.join(dir_path, the_file)\n",
    "            try:\n",
    "                if os.path.isfile(file_path) and (the_file.startswith('vk_group_topic')):\n",
    "                    cnt_t+=1\n",
    "                    print('tenant --> ',the_file)\n",
    "                    if cnt_t==1:\n",
    "                        df_tenant = pd.read_csv(file_path, sep=',', encoding='utf8')\n",
    "                    else:\n",
    "                        df_tmp = pd.read_csv(file_path, sep=',', encoding='utf8')\n",
    "                        df_tenant = df_tenant.append(df_tmp, ignore_index=True)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    elif len(findcat_landlord)!=0:\n",
    "        for the_file in os.listdir(dir_path):\n",
    "            file_path = os.path.join(dir_path, the_file)\n",
    "            try:\n",
    "                if os.path.isfile(file_path) and (the_file.startswith('vk_group_topic')):\n",
    "                    cnt_l+=1\n",
    "                    print('landlord --> ',the_file)\n",
    "                    if cnt_l==1:\n",
    "                        df_landlord = pd.read_csv(file_path, sep=',', encoding='utf8')\n",
    "                    else:\n",
    "                        df_tmp = pd.read_csv(file_path, sep=',', encoding='utf8')\n",
    "                        df_landlord = df_landlord.append(df_tmp, ignore_index=True)\n",
    "            except Exception as e:\n",
    "                print(e)                \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(df_tenant) , len(df_landlord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_landlord['LTA_flag'] = [1]*len(df_landlord)\n",
    "df_tenant['LTA_flag']   = [0]*len(df_tenant)\n",
    "df = pd.concat([df_landlord,df_tenant],ignore_index=True)\n",
    "df = shuffle(df)\n",
    "df.fillna('',inplace=True)\n",
    "df.rename(columns={'description':'SentimentText','LTA_flag':'Sentiment'},inplace=True)\n",
    "df = df[df.SentimentText!='']\n",
    "df.SentimentText = df.SentimentText.apply(lambda x: x[:4000])\n",
    "df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.groupby(['Sentiment'])['Sentiment'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_=0\n",
    "for indx in df.index:\n",
    "    len_ = len(df.loc[indx,'SentimentText'].split())\n",
    "    if len_ > max_:\n",
    "        max_ = len_\n",
    "        indx_ = indx\n",
    "print('max number of words in sample: {} with index: {}'.format(max_,indx_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpora preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b1b6a83e4a490f85c55e31e3fa98d6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df['cleaned_text'] = df.SentimentText.progress_apply(lambda x: normalize_text(preprocessing_setps,x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mstem = Mystem(mystem_bin='/home/mvp_dev/.local/bin/mystem')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization without parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['lemma_text'] = df.cleaned_text.progress_apply(lambda x: ''.join(mstem.lemmatize(x)[:-1] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use pandas multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "from multiprocessing import Pool\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemmatize_stem_all(df, col, proc_steps):\n",
    "    mstem = Mystem(mystem_bin='/home/mvp_dev/.local/bin/mystem')\n",
    "    df['lemma_text']=df[col].progress_apply(lambda x: ''.join(mstem.lemmatize(utils.normalize_text(proc_steps,x)[:-1])))\n",
    "def apply_func_to_df(df, col, proc_steps):\n",
    "    res = lemmatize_stem_all(df, col, proc_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_part = 3\n",
    "num_workers = 3\n",
    "def parallelize_df(df, func=apply_func_to_df):\n",
    "    df_split = np.array_split(df,num_part)\n",
    "    pool = Pool(num_workers)\n",
    "    df = pd.concat(pool.map(partial(func, col='SentimentText',proc_steps=utils.preprocessing_setps), df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = parallelize_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse lemmatized text to fix wrong class label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask = [True if (re.findall(r'снимать',x)!=[])and(re.findall(r'сдавать|сдать',x)==[]) else False for x in df.lemma_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(df[mask&df.Sentiment==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[mask&df.Sentiment==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reassing an appropriate class label for each masked corpus   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.loc[mask&df.Sentiment==1,'Sentiment']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.groupby(['Sentiment'])['Sentiment'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read saved dataframe with lemmatization and cleaned corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files=[]\n",
    "folder = os.path.join(os.getcwd(), 'data')\n",
    "for file_name in os.listdir(folder):\n",
    "    if file_name.startswith('topics'):\n",
    "        files.append(os.path.join(folder, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for file in files:\n",
    "    cnt+=1\n",
    "    if cnt == 1:\n",
    "        df_tmp = pd.read_csv(file, na_filter=False)\n",
    "        df = df_tmp\n",
    "    else:\n",
    "        df_tmp = pd.read_csv(file, na_filter=False)\n",
    "        df.append(df_tmp, ignore_index=True, sort=False)\n",
    "\n",
    "df.fillna('', inplace=True)\n",
    "# df = df[df.cleaned_text!='']\n",
    "# remove sentences consited of less than 3 words\n",
    "mask = np.array([True if len(ele.split())<=3 else False for ele in df['SentimentText']])\n",
    "df = df[~mask]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "390557"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Сдам комнаты под ключ в центре Феодосии в свое...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Молодая пара с кошечкой снимет комнату в двух ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Предлагаю койко-место в просторной, чистой,уют...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Сдам комнату в двухкомнатной квартире (субарен...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Сдам комнату в двухкомнатной квартире,20минут ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       SentimentText  Sentiment\n",
       "0  Сдам комнаты под ключ в центре Феодосии в свое...          0\n",
       "1  Молодая пара с кошечкой снимет комнату в двух ...          0\n",
       "2  Предлагаю койко-место в просторной, чистой,уют...          1\n",
       "3  Сдам комнату в двухкомнатной квартире (субарен...          1\n",
       "4  Сдам комнату в двухкомнатной квартире,20минут ...          1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate ngrams of subsentences from whole sample       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try some test cases with utils functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# res = extract_subsentences(line=df.cleaned_text[4], cutlenght=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# margin_sentences(df.cleaned_text[4],repeat_pattern=\"''\",returnlist=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# margin_sentences(cutlenght=10, line=extract_subsentences(line=df.cleaned_text[4], cutlenght=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run generation of subsentences from initial corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split df into batches to reduse memory consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_part = 3\n",
    "df_split = np.array_split(df,num_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "indx = 0\n",
    "df_split[indx]['subsentences'] = df_split[indx].cleaned_text.progress_apply(lambda x: margin_sentences(cutlenght=50,line=extract_sub_sentences(line=x, cutlenght=None)))\n",
    "df_split[indx].reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try without splitting but with reduced length of generated ngrams tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a86e1e1a4ac47aeaac7bcc1cfd4a257"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df['subsentences'] = df.cleaned_text.progress_apply(lambda x: margin_sentences(cutlenght=50,line=extract_subsentences(line=x, cutlenght=None)))\n",
    "df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize updated dataftame over `subsentences` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_fin = extend(df, ['subsentences'], fill_value='', preserve_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19527850"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mask = np.array([True if len(ele)==1 else False for ele in df_fin.subsentences])\n",
    "df_fin = df_fin[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9775881"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>subsentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Сдам комнаты под ключ в центре Феодосии в свое...</td>\n",
       "      <td>сдам комнаты ключ центре феодосии своем частно...</td>\n",
       "      <td>[сдам, комнаты]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Сдам комнаты под ключ в центре Феодосии в свое...</td>\n",
       "      <td>сдам комнаты ключ центре феодосии своем частно...</td>\n",
       "      <td>[сдам, комнаты, ключ, центре]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Сдам комнаты под ключ в центре Феодосии в свое...</td>\n",
       "      <td>сдам комнаты ключ центре феодосии своем частно...</td>\n",
       "      <td>[сдам, комнаты, ключ, центре, феодосии, своем]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Сдам комнаты под ключ в центре Феодосии в свое...</td>\n",
       "      <td>сдам комнаты ключ центре феодосии своем частно...</td>\n",
       "      <td>[сдам, комнаты, ключ, центре, феодосии, своем,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Сдам комнаты под ключ в центре Феодосии в свое...</td>\n",
       "      <td>сдам комнаты ключ центре феодосии своем частно...</td>\n",
       "      <td>[сдам, комнаты, ключ, центре, феодосии, своем,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                      SentimentText  \\\n",
       "0          0  Сдам комнаты под ключ в центре Феодосии в свое...   \n",
       "1          0  Сдам комнаты под ключ в центре Феодосии в свое...   \n",
       "2          0  Сдам комнаты под ключ в центре Феодосии в свое...   \n",
       "3          0  Сдам комнаты под ключ в центре Феодосии в свое...   \n",
       "4          0  Сдам комнаты под ключ в центре Феодосии в свое...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  сдам комнаты ключ центре феодосии своем частно...   \n",
       "1  сдам комнаты ключ центре феодосии своем частно...   \n",
       "2  сдам комнаты ключ центре феодосии своем частно...   \n",
       "3  сдам комнаты ключ центре феодосии своем частно...   \n",
       "4  сдам комнаты ключ центре феодосии своем частно...   \n",
       "\n",
       "                                        subsentences  \n",
       "0                                    [сдам, комнаты]  \n",
       "1                      [сдам, комнаты, ключ, центре]  \n",
       "2     [сдам, комнаты, ключ, центре, феодосии, своем]  \n",
       "3  [сдам, комнаты, ключ, центре, феодосии, своем,...  \n",
       "4  [сдам, комнаты, ключ, центре, феодосии, своем,...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fin.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save ot csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csvpath = Path.joinpath(Path(os.getcwd()), 'data', 'topics_all_subsentences_gzip.csv')\n",
    "# df_fin.to_csv(csvpath, index=False, encoding='utf8', compression=None)\n",
    "df  = pd.read_csv(csvpath, encoding='utf8', compression=None, na_filter=False, \n",
    "                  dtype={'SentimentText':str, 'cleaned_text':str, 'Sentiment':int,'subsentences':object})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csvpath = Path.joinpath(Path(os.getcwd()), '../../character-based-cnn/data', 'topics_all_lemma.csv')\n",
    "df[['SentimentText','Sentiment','lemma_text','cleaned_text','subsentences']].to_csv(csvpath, index=False, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frac_df = df.sample(frac=0.02)\n",
    "indx = pd.Series(df.index)\n",
    "mask = [not x for x in indx.isin(set(frac_df.index)).tolist()]\n",
    "df_train_test = df.iloc[mask,:]\n",
    "len(df_train_test), len(frac_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csvpath = Path.joinpath(Path(os.getcwd()), 'data', 'topics_all.csv')\n",
    "(df[['SentimentText','Sentiment']]).to_csv(csvpath, index=False, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csvpath = Path.joinpath(Path(os.getcwd()), '../../character-based-cnn/data', 'topics_train.csv')\n",
    "(df_train_test[['SentimentText','Sentiment']]).to_csv(csvpath, index=False, encoding='utf8')\n",
    "csvpath = Path.joinpath(Path(os.getcwd()), '../../character-based-cnn/data', 'topics_val.csv')\n",
    "(frac_df[['SentimentText','Sentiment']]).to_csv(csvpath, index=False, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5.3 (ZNO0059623792)",
   "language": "python",
   "name": "python35_zno0059623792"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
