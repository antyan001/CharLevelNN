{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.nn.functional\")\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "curruser = os.environ.get('USER')\n",
    "sys.path.insert(0, '/home/{}/python36-libs/lib/python3.6/site-packages/'.format(curruser))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import argparse\n",
    "import time\n",
    "import sys\n",
    "import pickle as pkl\n",
    "import string\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from torchviz import make_dot\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboardX import SummaryWriter\n",
    "import sentencepiece as spm\n",
    "# from bpe import Encoder\n",
    "from src.cnn_model import CharacterLevelLSTMCNN\n",
    "from src.data_loader import MyDataset\n",
    "from src import utils\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"{}\".format(str(torch._C._cuda_getDriverVersion()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, training_generator, optimizer, criterion, epoch, writer, print_every=1000):\n",
    "    \n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.nn.functional\")\n",
    "\n",
    "    model.train()\n",
    "    losses = []\n",
    "    accuraries = []\n",
    "    num_iter_per_epoch = len(training_generator)\n",
    "\n",
    "    progress_bar = tqdm(enumerate(training_generator),\n",
    "                                 total=num_iter_per_epoch,\n",
    "                                 file=sys.stdout)\n",
    "\n",
    "    for iter, batch in progress_bar:\n",
    "        features, labels = batch\n",
    "        if torch.cuda.is_available():\n",
    "            features = features.cuda()\n",
    "            labels = labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(features)\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_metrics = utils.get_evaluation(labels,\n",
    "                                                predictions,\n",
    "                                                list_metrics=[\"accuracy\", \"f1\"])\n",
    "        losses.append(loss.item())\n",
    "        accuraries.append(training_metrics[\"accuracy\"])\n",
    "        f1 = training_metrics['f1']\n",
    "               \n",
    "#         print(training_metrics['accuracy'])\n",
    "#         print(loss.item())\n",
    "#         print(f1)\n",
    "        \n",
    "        writer.add_scalar('Train/Loss', loss.item(),\n",
    "                          epoch * num_iter_per_epoch + iter)\n",
    "        writer.add_scalar(\n",
    "            'Train/Accuracy', training_metrics['accuracy'], epoch * num_iter_per_epoch + iter)\n",
    "        writer.add_scalar('Train/f1', f1,\n",
    "                          epoch * num_iter_per_epoch + iter)\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print(\"[Training - Epoch: {}] , Iteration: {}/{} , Loss: {}, Accuracy: {}\".format(\n",
    "                epoch + 1,\n",
    "                iter + 1,\n",
    "                num_iter_per_epoch,\n",
    "                np.mean(losses),\n",
    "                np.mean(accuraries)\n",
    "            ))\n",
    "            \n",
    "        progress_bar.update(1)\n",
    "\n",
    "    return np.mean(losses), np.mean(accuraries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, validation_generator, criterion, epoch, writer, print_every=500):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    accuraries = []\n",
    "    num_iter_per_epoch = len(validation_generator)\n",
    "    \n",
    "    progress_bar = tqdm(enumerate(validation_generator), \n",
    "                                 total=num_iter_per_epoch,\n",
    "                                 file=sys.stdout)\n",
    "    \n",
    "    for iter, batch in progress_bar:\n",
    "        features, labels = batch\n",
    "        if torch.cuda.is_available():\n",
    "            features = features.cuda()\n",
    "            labels = labels.cuda()\n",
    "        with torch.no_grad():\n",
    "            predictions = model(features)\n",
    "        loss = criterion(predictions, labels)\n",
    "        validation_metrics = utils.get_evaluation(labels,\n",
    "                                                  predictions,\n",
    "                                                  list_metrics=[\"accuracy\", \"f1\"])\n",
    "        accuracy = validation_metrics['accuracy']\n",
    "        f1 = validation_metrics['f1']\n",
    "        losses.append(loss.item())\n",
    "        accuraries.append(accuracy)\n",
    "\n",
    "        writer.add_scalar('Test/Loss', loss.item(),\n",
    "                          epoch * num_iter_per_epoch + iter)\n",
    "        writer.add_scalar('Test/Accuracy', accuracy,\n",
    "                          epoch * num_iter_per_epoch + iter)\n",
    "        writer.add_scalar('Test/f1', f1,\n",
    "                          epoch * num_iter_per_epoch + iter)\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print(\"[Validation - Epoch: {}] , Iteration: {}/{} , Loss: {}, Accuracy: {}\".format(\n",
    "                epoch + 1,\n",
    "                iter + 1,\n",
    "                num_iter_per_epoch,\n",
    "                np.mean(losses),\n",
    "                np.mean(accuraries)))\n",
    "            \n",
    "        progress_bar.update(1)\n",
    "\n",
    "    return np.mean(losses), np.mean(accuraries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run(args, both_cases=False):\n",
    "\n",
    "    log_path = args.log_path\n",
    "    if os.path.isdir(log_path):\n",
    "        shutil.rmtree(log_path)\n",
    "    os.makedirs(log_path)\n",
    "\n",
    "    if not os.path.exists(args.output):\n",
    "        os.makedirs(args.output)\n",
    "\n",
    "    writer = SummaryWriter(log_path)\n",
    "\n",
    "    batch_size = args.batch_size\n",
    "\n",
    "    training_params = {\"batch_size\": batch_size,\n",
    "                       \"shuffle\": True,\n",
    "                       \"num_workers\": args.workers}\n",
    "\n",
    "    validation_params = {\"batch_size\": batch_size,\n",
    "                         \"shuffle\": False,\n",
    "                         \"num_workers\": args.workers} \n",
    "    \n",
    "    ##########################################################\n",
    "    # Calculate vocab size for ngram tokenizer\n",
    "    # And obtain a mapping of ngram vectors on their ids\n",
    "    ##########################################################\n",
    "    trigram_map, vocab_size=MyDataset._gen_ngrams()\n",
    "    args.trigram_map_len = vocab_size+1\n",
    "    \n",
    "    #############################################################\n",
    "    # if useBOCNGrams is True then calculate maxlen variable \n",
    "    # corresponding to the total number of ngrams to be generated \n",
    "    # from input rawdata\n",
    "    #############################################################\n",
    "    maxSeqLen = 120\n",
    "    maxlen = np.sum([(maxSeqLen-ker)+1 for ker in range(1,5)]) \n",
    "    \n",
    "    #############################################################\n",
    "    # Calculate vocab size for different pretrained Tokenizers \n",
    "    #############################################################\n",
    "    if args.useSentencePieceTokenizer:\n",
    "        sp = spm.SentencePieceProcessor()\n",
    "        sp.Load(args.data_path_to_SentpBPE)            \n",
    "        args.vocabSize = sp.GetPieceSize()\n",
    "    elif args.useNGramBPETokenizer:\n",
    "        bpe = Encoder.load(args.data_path_to_NGramBPE)\n",
    "        args.vocabSize = bpe.vocab_size\n",
    "        \n",
    "    #############################################################\n",
    "    # MAIN: Model instantiation and params initialization\n",
    "    #############################################################\n",
    "    model = CharacterLevelLSTMCNN(args)\n",
    "    # Save Embed weights onto class atribute\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()  \n",
    "    if args.usembedding:    \n",
    "        args._embedding = model._embedding.Embedding.weight.data.cpu().numpy()  \n",
    "    else:\n",
    "        args._embedding = None\n",
    "    full_dataset = MyDataset(args)\n",
    "    ##########################################################\n",
    "    \n",
    "    train_size = int(args.validation_split * len(full_dataset))\n",
    "    validation_size = len(full_dataset) - train_size\n",
    "    training_set, validation_set = torch.utils.data.random_split(\n",
    "        full_dataset, [train_size, validation_size])\n",
    "    training_generator = DataLoader(training_set, **training_params)\n",
    "    validation_generator = DataLoader(validation_set, **validation_params)\n",
    "\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if args.optimizer == 'sgd':\n",
    "        optimizer = torch.optim.SGD(\n",
    "            model.parameters(), lr=args.learning_rate, momentum=0.9\n",
    "        )\n",
    "    elif args.optimizer == 'adam':\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(), lr=args.learning_rate\n",
    "        )\n",
    "\n",
    "    best_loss = 1e10\n",
    "    best_epoch = 0\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        training_loss, training_accuracy = train(model,\n",
    "                                                 training_generator,\n",
    "                                                 optimizer,\n",
    "                                                 criterion,\n",
    "                                                 epoch,\n",
    "                                                 writer)\n",
    "\n",
    "        validation_loss, validation_accuracy = evaluate(model,\n",
    "                                                        validation_generator,\n",
    "                                                        criterion,\n",
    "                                                        epoch,\n",
    "                                                        writer)\n",
    "\n",
    "        print('[Epoch: {} / {}]\\ttrain_loss: {:.4f} \\ttrain_acc: {:.4f} \\tval_loss: {:.4f} \\tval_acc: {:.4f}'.\n",
    "              format(epoch + 1, args.epochs, training_loss, training_accuracy, validation_loss, validation_accuracy))\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # learning rate scheduling\n",
    "\n",
    "        if args.schedule != 0:\n",
    "            if args.optimizer == 'sgd' and epoch % args.schedule == 0 and epoch > 0:\n",
    "                current_lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "                current_lr /= 2\n",
    "                print('Decreasing learning rate to {0}'.format(current_lr))\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = current_lr\n",
    "\n",
    "        #  valiearly stopping\n",
    "        if validation_loss < best_loss:\n",
    "            best_loss = validation_loss\n",
    "            best_epoch = epoch\n",
    "            if args.checkpoint == 1:\n",
    "                torch.save(model, args.output + '{}_epoch_{}_lr_{}_loss_{}_acc_{}.pth'.format(args.model_name,\n",
    "                                                                                                    epoch,\n",
    "                                                                                                    optimizer.state_dict()[\n",
    "                                                                                                        'param_groups'][0]['lr'],\n",
    "                                                                                                    round(\n",
    "                                                                                                        validation_loss, 4),\n",
    "                                                                                                    round(\n",
    "                                                                                                        validation_accuracy, 4)\n",
    "                                                                                                    ))\n",
    "\n",
    "        if epoch - best_epoch > args.patience > 0:\n",
    "            print(\"Stop training at epoch {}. The lowest loss achieved is {} at epoch {}\".format(\n",
    "                epoch, validation_loss, best_epoch))\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSeqLen = 120\n",
    "maxlen = np.sum([(maxSeqLen-ker)+1 for ker in range(1,5)]) \n",
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        'Character Based CNN for text classification')\n",
    "    parser.add_argument('--data_path', type=str, default='./data/dnaseg/csv/DnaSeg4TrainwLabelsSampled_wRandL.csv')\n",
    "    parser.add_argument('--validation_split', type=float, default=0.95)\n",
    "    parser.add_argument('--label_column', type=str, default='label')\n",
    "    parser.add_argument('--text_column', type=str, default='seq')\n",
    "    parser.add_argument('--max_rows', type=int, default=6000000)\n",
    "    parser.add_argument('--chunksize', type=int, default=1000000)\n",
    "    parser.add_argument('--encoding', type=str, default='utf8')\n",
    "    parser.add_argument('--sep', type=str, default=';')\n",
    "    parser.add_argument('--steps', nargs='+', default=None)\n",
    "    parser.add_argument('--alphabet', type=str, default=\"\"\"ACGT\"\"\") #char_indices: {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "    parser.add_argument('--number_of_characters', type=int, default=4)\n",
    "    parser.add_argument('--extra_characters', type=str, default=[])\n",
    "    parser.add_argument('--config_path', type=str, default='./config.json')\n",
    "    parser.add_argument('--size', type=str,\n",
    "                        choices=['small', 'large'], default='small')\n",
    "    #if useBOCNGrams is True then set param equals to maxlen variable otherwise choose some int value\n",
    "    parser.add_argument('--max_length', type=int, default=maxlen)\n",
    "    # Add a new signal with the use of Conv1D operator over filters rather than over sequence channel\n",
    "    parser.add_argument('--useSeparatedConv1D', type=bool, default=True)\n",
    "    # Decide to add LSTM signal or not\n",
    "    parser.add_argument('--useLSTM', type=bool, default=False)\n",
    "    # Decide wether to choose a Conv1D operator for dimensionality reduction\n",
    "    # Or apply AvgPool1D operator (with fixed moving window size determined by para LSTMAvgPoolKernelSize)\n",
    "    parser.add_argument('--applyConv1DForLSTM', type=bool, default=False)\n",
    "    parser.add_argument('--setLSTMAvgPoolKernelSize', type=int, default=30)\n",
    "    # Shuffle dims of LSTM output tensor and peform a Conv1D operation over sequence space channel    \n",
    "    parser.add_argument('--changeConv1DDirLSTM', type=bool, default=False)\n",
    "    # Use pretrained SentencePiece tokenizer with Unigram/BPE model \n",
    "    parser.add_argument('--useSentencePieceTokenizer', type=bool, default=False)\n",
    "    parser.add_argument('--data_path_to_SentpBPE', type=str, default=\"models/BPE/sentpbpe.model\")\n",
    "    # Use pretrained BPE tokenizer with controlable range of ngrams\n",
    "    parser.add_argument('--useNGramBPETokenizer', type=bool, default=False)\n",
    "    parser.add_argument('--data_path_to_NGramBPE', type=str, default=\"models/BPE/bpe.model\")\n",
    "    # Enable word hashing with uni/bi/trigrams tokenization approach\n",
    "    parser.add_argument('--useBOCNGrams', type=bool, default=True)\n",
    "    # Embedding usage\n",
    "    parser.add_argument('--usembedding', type=bool, default=False)\n",
    "    # Subsitute One-Hot to relevant Embedding on Forward pass after yielding data from DataGenerator\n",
    "    # Else transform One-Hot to its Embed vector on the stage of Data Loading  \n",
    "    parser.add_argument('--embedAfterBatches', type=bool, default=False) \n",
    "    parser.add_argument('--embedlength', type=int, default=100)\n",
    "    parser.add_argument('--number_of_classes', type=int, default=4)\n",
    "    parser.add_argument('--epochs', type=int, default=140)\n",
    "    parser.add_argument('--batch_size', type=int, default=1024)\n",
    "    parser.add_argument('--useBatchNormalization', type=bool, default=True)\n",
    "    parser.add_argument('--optimizer', type=str,\n",
    "                        choices=['adam', 'sgd'], default='sgd')\n",
    "    parser.add_argument('--learning_rate', type=float, default=1e-2)\n",
    "    parser.add_argument('--schedule', type=int, default=20)\n",
    "    parser.add_argument('--patience', type=int, default=10)\n",
    "    parser.add_argument('--checkpoint', type=int, choices=[0, 1], default=1)\n",
    "    parser.add_argument('--workers', type=int, default=8)\n",
    "    parser.add_argument('--log_path', type=str, default='./logs/')\n",
    "    parser.add_argument('--output', type=str, default='./models/torch/')\n",
    "    parser.add_argument('--model_name', type=str, default='char_cnn_bpe')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_interactive():\n",
    "    return not hasattr(sys.modules['__main__'], '__file__')\n",
    "\n",
    "# work-around for Jupyter notebook and IPython console\n",
    "argv = [] if is_interactive() else sys.argv[1:]\n",
    "args = parser.parse_args(argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create output directory in project root\n",
    "ROOT_DIR = os.path.abspath(os.getcwd())\n",
    "OUTPUT_DIR = os.path.join(ROOT_DIR, \"torch_output\")\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Test Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##########################################################\n",
    "trigram_map, vocab_size=MyDataset._gen_ngrams()\n",
    "args.trigram_map_len = vocab_size+1\n",
    "\n",
    "#############################################################\n",
    "# Calculate vocab size for different pretrained Tokenizers \n",
    "#############################################################\n",
    "if args.useSentencePieceTokenizer:\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.Load(args.data_path_to_SentpBPE)            \n",
    "    args.vocabSize = sp.GetPieceSize()\n",
    "elif args.useNGramBPETokenizer:\n",
    "    bpe = Encoder.load(args.data_path_to_NGramBPE)\n",
    "    args.vocabSize = bpe.vocab_size\n",
    "        \n",
    "model = CharacterLevelLSTMCNN(args)\n",
    "\n",
    "# args._embedding = model._embedding  \n",
    "\n",
    "# full_dataset = MyDataset(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# input = torch.rand((1024,400,100))\n",
    "# input = input.to(model.device)\n",
    "# out = model.forward(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# make_dot(out).render(\"./img/dcnn_graph_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dict(model.named_parameters())['_embedding.Embedding.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if torch.cuda.is_available():\n",
    "#     model.cuda()  \n",
    "# if args.usembedding:    \n",
    "#     args._embedding = model._embedding.weight.data.cpu().numpy()  \n",
    "# else:\n",
    "#     args._embedding = None\n",
    "\n",
    "# full_dataset = MyDataset(args)\n",
    "# x, y = full_dataset[100]\n",
    "# lst = [np.any(x[i]) for i in range(x.shape[0])]\n",
    "# len(lst) -1 - lst[::-1].index(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from torch.multiprocessing import set_start_method\n",
    "# try:\n",
    "#     set_start_method('spawn')\n",
    "# except RunTimeError:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.3 (GPUAI)",
   "language": "python",
   "name": "python36_gpuai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
