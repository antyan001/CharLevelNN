{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "curruser = os.environ.get('USER')\n",
    "sys.path.insert(0, './src/')\n",
    "# sys.path.insert(0, '/home/{}/notebooks/support_library/'.format(curruser)) \n",
    "# sys.path.insert(0, '/home/{}/python36-libs/lib/python3.6/site-packages/'.format(curruser))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(42)\n",
    "# import keras as K\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Dense, Activation, Permute\n",
    "from keras.layers import LSTM, Dropout, CuDNNGRU, Bidirectional\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.core import Dense, Activation, Dropout, RepeatVector\n",
    "from keras.optimizers import RMSprop, Adam, Adagrad, Adadelta, SGD\n",
    "from keras.layers import Input, Flatten, Dense, Dropout, Convolution1D, Conv1D, \\\n",
    "                         MaxPool1D, Lambda, GlobalMaxPooling1D, GlobalAveragePooling1D, \\\n",
    "                         BatchNormalization, Activation, AveragePooling1D, Concatenate, concatenate\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, \\\n",
    "                                    KFold, StratifiedKFold, RepeatedKFold, RepeatedStratifiedKFold\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
    "from keras.wrappers.scikit_learn import KerasRegressor, KerasClassifier\n",
    "from keras.utils import np_utils, to_categorical\n",
    "import keras\n",
    "import math        \n",
    "        \n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import sys\n",
    "import heapq\n",
    "import seaborn as sns\n",
    "from pylab import rcParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "rcParams['figure.figsize'] = 12, 5\n",
    "\n",
    "\n",
    "def plot_metrics(history):\n",
    "    plt.plot(history['acc'])\n",
    "    plt.plot(history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with K.tf.device('/GPU:0'):\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "    session = tf.Session(config=config)\n",
    "    K.set_session(session)\n",
    "#     set_gpu_option(\"0\", 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "def set_gpu_option(which_gpu, fraction_memory):\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = fraction_memory\n",
    "    config.gpu_options.visible_device_list = which_gpu\n",
    "    set_session(tf.Session(config=config))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preventing TF from allocating the totally of a GPU memory\n",
    "# gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.4)\n",
    "# sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set_gpu_option(\"0\", 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "csvpath = Path.joinpath(Path(os.getcwd()),'data/dnaseg/csv/','DnaSeg4TrainwLabelsSampled_wRandL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(csvpath, delimiter=';', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ATATGTATTTTCTTTTTGTGGAGAGCATTTTTCCCTCGTGATTACA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TGGAGGGGAAAAGCCAGGAGACCTCCGAGCTTGCACATATTGTAGA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TTAACCTTGCAAGAACTCTTCAGGCACATATGGAAGATCTCG</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TCTGAAGTAAATTATATCATTGAAAGACCAAGCTACCCTCTGAAGA...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GACTTGCATACCAACATAATCAGACCGTCTGCAGAAATTCTCCTAC...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 seq  label\n",
       "0     ATATGTATTTTCTTTTTGTGGAGAGCATTTTTCCCTCGTGATTACA      0\n",
       "1     TGGAGGGGAAAAGCCAGGAGACCTCCGAGCTTGCACATATTGTAGA      0\n",
       "2         TTAACCTTGCAAGAACTCTTCAGGCACATATGGAAGATCTCG      0\n",
       "3  TCTGAAGTAAATTATATCATTGAAAGACCAAGCTACCCTCTGAAGA...      0\n",
       "4  GACTTGCATACCAACATAATCAGACCGTCTGCAGAAATTCTCCTAC...      0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(778376, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't Forget to Shuffle Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seqlst = df.seq.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = sorted(list(set(seqlst[0])))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0, 'C': 1, 'G': 2, 'T': 3}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create one-hot encoded vectors using the `char_indices` map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = df.seq.str.len().max()\n",
    "step = 5\n",
    "\n",
    "sentences = df.seq.tolist()\n",
    "next_chars = df.label.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "778376it [00:14, 54368.45it/s]\n"
     ]
    }
   ],
   "source": [
    "X = np.zeros((len(sentences), SEQUENCE_LENGTH, len(chars)),dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in tqdm(enumerate(sentences)):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "        #y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(next_chars, num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(778376, 119, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del sentences, next_chars, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDC - User Defined Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Schedules and Adaptive Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.1\n",
    "    drop = 0.5\n",
    "    epochs_drop = 4.0\n",
    "    lrate = initial_lrate * math.pow(drop,  \n",
    "           math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "       self.losses = []\n",
    "       self.lr = []\n",
    " \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "       self.losses.append(logs.get('loss'))\n",
    "       self.lr.append(step_decay(len(self.losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = LossHistory()\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "callbacks_list = [loss_history, lrate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_callbacks(name_weights, patience_lr):\n",
    "    mcp_save = ModelCheckpoint(name_weights, save_best_only=True, monitor='val_loss', mode='min')\n",
    "    reduce_lr_loss = ReduceLROnPlateau(monitor='loss', factor=0.01, patience=patience_lr, \n",
    "                                       verbose=1, epsilon=1e-4, mode='min')\n",
    "    return [mcp_save, reduce_lr_loss]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Multi-Layered LSTM with TimeDistributed SubLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# model.add(LSTM(units=128, input_shape=(2*SEQUENCE_LENGTH, len(chars)),\n",
    "#                dropout=0.2, recurrent_dropout=0.2))\n",
    "# model.add(Permute((2,1), input_shape=(2*SEQUENCE_LENGTH, len(chars))))\n",
    "model.add(LSTM(units=128, input_shape=(2*SEQUENCE_LENGTH, len(chars)), \n",
    "               return_sequences=True,kernel_initializer = \"he_uniform\", use_bias = True,\n",
    "               dropout=0.4, recurrent_dropout=0.4, activation=\"tanh\"))\n",
    "# model.add(TimeDistributed(Dense(10, input_shape = (2*SEQUENCE_LENGTH, 128), activation=\"tanh\")))\n",
    "model.add(LSTM(units=64, dropout=0.2, recurrent_dropout=0.2, activation=\"tanh\", use_bias = True))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "indx = np.random.choice(np.arange(len(X)), size=1000000)\n",
    "name_weights = \"./models/keras/final_model\" + \"_weights.h5\"\n",
    "callbacks = get_callbacks(name_weights = name_weights, patience_lr=4)\n",
    "    \n",
    "# optimizer = RMSprop(lr=0.06)\n",
    "optimizer = Adam(lr=0.01)\n",
    "# optimizer = Adagrad(lr=0.02, decay=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "history = model.fit(X[indx], y[indx], \n",
    "                    validation_split=0.05, \n",
    "                    batch_size=1024, \n",
    "                    epochs=10,\n",
    "                    callbacks=callbacks,\n",
    "                    shuffle=True).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_metrics(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to Outperform Targeted Metrics Using Cross-Val Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name_weights = \"./models/keras/final_model_fold\" + \"_weights.h5\"\n",
    "callbacks = get_callbacks(name_weights = name_weights, patience_lr=4)\n",
    "\n",
    "# optimizer = RMSprop(lr=0.06)\n",
    "# optimizer = Adam(lr=0.01)\n",
    "optimizer = Adagrad(lr=0.05, decay=0.01)\n",
    "\n",
    "def buildmodel():\n",
    "    model = Sequential()\n",
    "    # model.add(LSTM(units=128, input_shape=(2*SEQUENCE_LENGTH, len(chars)),\n",
    "    #                dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(LSTM(units=128, input_shape=(2*SEQUENCE_LENGTH, len(chars)), return_sequences=True,\n",
    "                   dropout=0.3, recurrent_dropout=0.3))\n",
    "    model.add(TimeDistributed(Dense(10, input_shape = (2*SEQUENCE_LENGTH,128), activation=\"relu\")))\n",
    "    model.add(LSTM(units=64, dropout=0.4, recurrent_dropout=0.4))\n",
    "    model.add(Dense(units=32, activation='relu'))\n",
    "    model.add(Dense(len(chars)))\n",
    "    model.add(Activation('softmax'))    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "buildmodel().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ScikitWrapper4Keras: Implementation of the scikit-learn classifier API for Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=buildmodel, epochs=15, batch_size=1024, verbose=1)\n",
    "kfold = KFold(n_splits=4, shuffle=True)\n",
    "results = cross_val_score(estimator, X, y, cv=kfold, n_jobs=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Different Folding Techniques To Achieve Best Metrics (by preserving the percentage of samples for each class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folds = RepeatedStratifiedKFold(n_splits=4, n_repeats=20, random_state=45).split(X, y.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size=1024\n",
    "\n",
    "for j, (train_idx, val_idx) in enumerate(folds):\n",
    "    \n",
    "    print('Fold: {}'.format(j))\n",
    "    X_train_cv = X[train_idx]\n",
    "    y_train_cv = y[train_idx]\n",
    "    X_valid_cv = X[val_idx]\n",
    "    y_valid_cv = y[val_idx]\n",
    "    \n",
    "    buildmodel().fit(\n",
    "                X_train_cv,\n",
    "                y_train_cv,\n",
    "                batch_size = batch_size,\n",
    "                #steps_per_epoch = int(len(X_train_cv)/batch_size),\n",
    "                #validation_steps = int(len(X_valid_cv)/batch_size),\n",
    "                epochs=15,\n",
    "                shuffle=True,\n",
    "                verbose=1,\n",
    "                validation_data = (X_valid_cv, y_valid_cv),\n",
    "                callbacks = callbacks)\n",
    "    \n",
    "    print(buildmodel().evaluate(X_valid_cv, y_valid_cv))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Straight-Forward STACKING: Try to perform a Stacking of different LSTM layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(units=128, input_shape=(2*SEQUENCE_LENGTH, len(chars)), return_sequences=True,\n",
    "                                       dropout=0.4, recurrent_dropout=0.4))\n",
    "model.add(LSTM(units=64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(LSTM(units=32, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# optimizer = RMSprop(lr=0.01)\n",
    "optimizer = Adagrad(lr=0.1, decay=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "history = model.fit(X, y, validation_split=0.05, batch_size=128, epochs=10, shuffle=True).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_metrics(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM+FCN for Time Series Classification (Fully Convolutional Block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./img/20181125-lstm-fcn_architecture.png \"LSTM FCN for Time Series Classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_CLASSES = 4\n",
    "SEQUENCE_LENGTH = X.shape[1]\n",
    "SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_fcn_block():\n",
    "    ip    = Input(shape = (SEQUENCE_LENGTH, NB_CLASSES))\n",
    "#     iphat = Permute((2,1), input_shape=(2*SEQUENCE_LENGTH, NB_CLASSES))(ip)\n",
    "    x = LSTM(units=128, input_shape=(SEQUENCE_LENGTH, NB_CLASSES), \n",
    "             return_sequences=True, kernel_initializer = \"he_uniform\",\n",
    "             dropout=0.4, recurrent_dropout=0.4, activation=\"tanh\")(ip)\n",
    "#     for i in range(3):\n",
    "#         if i==0:\n",
    "#             x = LSTM(units=128, input_shape=(SEQUENCE_LENGTH, NB_CLASSES), \n",
    "#                      return_sequences=True, kernel_initializer = \"he_uniform\",\n",
    "#                      dropout=0.4, recurrent_dropout=0.4, activation=\"tanh\")(ip)   \n",
    "# #             x = TimeDistributed(Dense(10, input_shape = (SEQUENCE_LENGTH,128), activation=\"tanh\"))(x)            \n",
    "#         else:\n",
    "#             x = Concatenate()([x, ip])\n",
    "#             x = LSTM(units=128, \n",
    "#                      return_sequences=True, kernel_initializer = \"he_uniform\",\n",
    "#                      dropout=0.4, recurrent_dropout=0.4, activation=\"tanh\")(x)\n",
    "\n",
    "    x = LSTM(units=64, dropout=0.4, recurrent_dropout=0.4, \n",
    "             kernel_initializer = \"he_uniform\", activation=\"tanh\")(x)\n",
    "#     x = Dense(units=32, activation='relu')\n",
    "\n",
    "    rnn, rnn_fw, rnn_bw = Bidirectional(CuDNNGRU(100, return_sequences=False, return_state=True))(ip)\n",
    "    \n",
    "#     iphat = Permute((2,1), input_shape=(SEQUENCE_LENGTH, NB_CLASSES))(ip)\n",
    "    y = Conv1D(filters=256, kernel_size=3, strides=1, \n",
    "               padding='valid', use_bias=True, \n",
    "#                data_format='channels_first', \n",
    "               kernel_initializer = \"he_uniform\", \n",
    "               name='c1d_1')(ip)\n",
    "    y = Activation(\"tanh\")(y)\n",
    "    y = MaxPool1D(pool_size=2)(y)    \n",
    "#     y = MaxPool1D(pool_size=2, data_format='channels_first')(y)\n",
    "#     y = BatchNormalization()(y)\n",
    "    \n",
    "    y = Conv1D(filters=256, kernel_size=3, strides=1, \n",
    "               padding='valid', use_bias=True, \n",
    "#                data_format='channels_first', \n",
    "               kernel_initializer = \"he_uniform\", \n",
    "               name='c1d_2')(y)\n",
    "    y = Activation(\"relu\")(y)\n",
    "#     y = MaxPool1D(pool_size=3, data_format='channels_first')(y)\n",
    "#     y = MaxPool1D(pool_size=2)(y)\n",
    "#     y = BatchNormalization()(y)\n",
    "    \n",
    "    y = Conv1D(filters=256, kernel_size=3, strides=1, \n",
    "               padding='valid', use_bias=True, \n",
    "#                data_format='channels_first', \n",
    "               kernel_initializer = \"he_uniform\", \n",
    "               name='c1d_3')(y)\n",
    "    y = Activation(\"tanh\")(y)\n",
    "#     y = MaxPool1D(pool_size=2, data_format='channels_first')(y)\n",
    "    y = MaxPool1D(pool_size=2)(y)\n",
    "#     y = BatchNormalization()(y)    \n",
    "    \n",
    "    y = Conv1D(filters=128, kernel_size=3, strides=1, \n",
    "               padding='valid', use_bias=True, \n",
    "#                data_format='channels_first', \n",
    "               kernel_initializer = \"he_uniform\", \n",
    "               name='c1d_4')(y)\n",
    "    y = Activation(\"relu\")(y)\n",
    "#     y = BatchNormalization()(y)\n",
    "    \n",
    "    y = Flatten()(y)\n",
    "#     y = GlobalAveragePooling1D()(y)\n",
    "    y = Dropout(0.5)(y)\n",
    "    x = Concatenate()([x, y, rnn])\n",
    "\n",
    "    x = Dense(1024, activation = \"relu\")(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(1024, activation = \"relu\")(x)\n",
    "    x = Dropout(0.3)(x)    \n",
    "    \n",
    "    out = Dense(NB_CLASSES, activation = \"softmax\")(x)\n",
    "\n",
    "    model = Model(ip, out)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 119, 4)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "c1d_1 (Conv1D)                  (None, 117, 256)     3328        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 117, 256)     0           c1d_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 58, 256)      0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "c1d_2 (Conv1D)                  (None, 56, 256)      196864      max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 56, 256)      0           c1d_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "c1d_3 (Conv1D)                  (None, 54, 256)      196864      activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 54, 256)      0           c1d_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 27, 256)      0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "c1d_4 (Conv1D)                  (None, 25, 128)      98432       max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 25, 128)      0           c1d_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (None, 119, 128)     68096       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 3200)         0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   (None, 64)           49408       lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 3200)         0           flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) [(None, 200), (None, 63600       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 3464)         0           lstm_6[0][0]                     \n",
      "                                                                 dropout_4[0][0]                  \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1024)         3548160     concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 1024)         0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1024)         1049600     dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 1024)         0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 4)            4100        dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 5,278,452\n",
      "Trainable params: 5,278,452\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = lstm_fcn_block()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 700538 samples, validate on 77838 samples\n",
      "Epoch 1/140\n",
      "700538/700538 [==============================] - 1362s 2ms/step - loss: 1.3801 - acc: 0.2854 - val_loss: 1.3758 - val_acc: 0.2982\n",
      "Epoch 2/140\n",
      "700538/700538 [==============================] - 1361s 2ms/step - loss: 1.3774 - acc: 0.2919 - val_loss: 1.3752 - val_acc: 0.2997\n",
      "Epoch 3/140\n",
      "700538/700538 [==============================] - 1363s 2ms/step - loss: 1.3769 - acc: 0.2935 - val_loss: 1.3749 - val_acc: 0.2997\n",
      "Epoch 4/140\n",
      "700538/700538 [==============================] - 1361s 2ms/step - loss: 1.3765 - acc: 0.2944 - val_loss: 1.3745 - val_acc: 0.3009\n",
      "Epoch 5/140\n",
      "700538/700538 [==============================] - 1362s 2ms/step - loss: 1.3762 - acc: 0.2953 - val_loss: 1.3743 - val_acc: 0.3012\n",
      "Epoch 6/140\n",
      "700538/700538 [==============================] - 1364s 2ms/step - loss: 1.3760 - acc: 0.2952 - val_loss: 1.3742 - val_acc: 0.3020\n",
      "Epoch 7/140\n",
      "700538/700538 [==============================] - 1368s 2ms/step - loss: 1.3759 - acc: 0.2957 - val_loss: 1.3740 - val_acc: 0.3026\n",
      "Epoch 8/140\n",
      "700538/700538 [==============================] - 1367s 2ms/step - loss: 1.3755 - acc: 0.2953 - val_loss: 1.3738 - val_acc: 0.3035\n",
      "Epoch 9/140\n",
      "700538/700538 [==============================] - 1366s 2ms/step - loss: 1.3755 - acc: 0.2961 - val_loss: 1.3736 - val_acc: 0.3034\n",
      "Epoch 10/140\n",
      "700538/700538 [==============================] - 1368s 2ms/step - loss: 1.3754 - acc: 0.2967 - val_loss: 1.3737 - val_acc: 0.3026\n",
      "Epoch 11/140\n",
      "700538/700538 [==============================] - 1365s 2ms/step - loss: 1.3751 - acc: 0.2970 - val_loss: 1.3734 - val_acc: 0.3041\n",
      "Epoch 12/140\n",
      "700538/700538 [==============================] - 1367s 2ms/step - loss: 1.3751 - acc: 0.2963 - val_loss: 1.3733 - val_acc: 0.3038\n",
      "Epoch 13/140\n",
      "318208/700538 [============>.................] - ETA: 12:08 - loss: 1.3749 - acc: 0.2979"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-d4c48a687785>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m140\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     shuffle=True).history\n\u001b[0m",
      "\u001b[0;32m/opt/cloudera/parcels/PYENV.GPUAI/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/opt/cloudera/parcels/PYENV.GPUAI/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/PYENV.GPUAI/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/PYENV.GPUAI/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/PYENV.GPUAI/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/PYENV.GPUAI/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/PYENV.GPUAI/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/PYENV.GPUAI/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# indx = np.random.choice(np.arange(len(X)), size=6000000)\n",
    "name_weights = \"./models/keras/cnn_bilstm_model.h5\"\n",
    "callbacks = get_callbacks(name_weights = name_weights, patience_lr=4)\n",
    "    \n",
    "optimizer = SGD(lr=5e-2, momentum=0.5, decay=1e-2, nesterov=True)    \n",
    "# optimizer = RMSprop(lr=0.04)\n",
    "# optimizer = Adam(lr=5e-2, decay=5e-3)\n",
    "# optimizer = Adadelta(lr=5e-2, decay=5e-3)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "history = model.fit(X[:], y[:], \n",
    "                    validation_split=0.1, \n",
    "                    batch_size=256, \n",
    "                    epochs=140,\n",
    "                    callbacks=callbacks,\n",
    "                    shuffle=True).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('./models/keras/bilstm_fcnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN with FCL Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_fcn_block():\n",
    "    ip    = Input(shape = (2*SEQUENCE_LENGTH, NB_CLASSES))\n",
    "    iphat = Permute((2,1), input_shape=(2*SEQUENCE_LENGTH, NB_CLASSES))(ip)\n",
    "    y = Conv1D(filters=256, kernel_size=3, strides=1, \n",
    "               padding='valid', use_bias=True, \n",
    "               data_format='channels_first', \n",
    "               kernel_initializer = \"he_uniform\", \n",
    "               name='c1d_1')(iphat)\n",
    "    y = Activation(\"relu\")(y)\n",
    "    y = MaxPool1D(pool_size=2, data_format='channels_first')(y)\n",
    "#     y = BatchNormalization()(y)\n",
    "    \n",
    "    y = Conv1D(filters=256, kernel_size=3, strides=1, \n",
    "               padding='valid', use_bias=True, \n",
    "               data_format='channels_first', \n",
    "               kernel_initializer = \"he_uniform\", \n",
    "               name='c1d_2')(y)\n",
    "    y = Activation(\"relu\")(y)\n",
    "    y = MaxPool1D(pool_size=2, data_format='channels_first')(y)\n",
    "#     y = BatchNormalization()(y)\n",
    "    \n",
    "    y = Conv1D(filters=256, kernel_size=5, strides=1, \n",
    "               padding='valid', use_bias=True, \n",
    "               data_format='channels_first', \n",
    "               kernel_initializer = \"he_uniform\", \n",
    "               name='c1d_3')(y)\n",
    "    y = Activation(\"relu\")(y)\n",
    "#     y = MaxPool1D(pool_size=2, data_format='channels_first')(y)\n",
    "#     y = BatchNormalization()(y)    \n",
    " \n",
    "    y = Conv1D(filters=256, kernel_size=2, strides=1, \n",
    "               padding='valid', use_bias=True, \n",
    "               data_format='channels_first', \n",
    "               kernel_initializer = \"he_uniform\", \n",
    "               name='c1d_4')(y)\n",
    "    y = Activation(\"relu\")(y)    \n",
    "    \n",
    "    y = Conv1D(filters=256, kernel_size=2, strides=1, \n",
    "               padding='valid', use_bias=True, \n",
    "               data_format='channels_first', \n",
    "               kernel_initializer = \"he_uniform\", \n",
    "               name='c1d_5')(y)\n",
    "    y = Activation(\"relu\")(y)\n",
    "    \n",
    "    y = Conv1D(filters=256, kernel_size=3, strides=1, \n",
    "               padding='valid', use_bias=True, \n",
    "               data_format='channels_first', \n",
    "               kernel_initializer = \"he_uniform\", \n",
    "               name='c1d_6')(y)\n",
    "    y = Activation(\"relu\")(y)\n",
    "    y = MaxPool1D(pool_size=2, data_format='channels_first')(y)    \n",
    "#     y = BatchNormalization()(y)\n",
    "    \n",
    "    y = Flatten()(y)\n",
    "#     y = GlobalAveragePooling1D()(y)\n",
    "\n",
    "    y = Dense(1024, activation = \"relu\")(y)\n",
    "    y = Dropout(0.5)(y)\n",
    "    y = Dense(1024, activation = \"relu\")(y)\n",
    "    y = Dropout(0.5)(y)\n",
    "    out = Dense(NB_CLASSES, activation = \"softmax\")(y)\n",
    "\n",
    "    model = Model(ip, out)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = lstm_fcn_block()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# indx = np.random.choice(np.arange(len(X)), size=6000000)\n",
    "name_weights = \"./models/keras/final_model\" + \"_weights.h5\"\n",
    "callbacks = get_callbacks(name_weights = name_weights, patience_lr=4)\n",
    "    \n",
    "optimizer = SGD(lr=0.1, nesterov=True)\n",
    "# optimizer = Adam(lr=0.03)\n",
    "# optimizer = Adadelta(lr=0.1, decay=0.0)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['categorical_accuracy'])\n",
    "history = model.fit(X[:], y[:], \n",
    "                    validation_split=0.05, \n",
    "                    batch_size=512, \n",
    "                    epochs=40,\n",
    "                    callbacks=callbacks,\n",
    "                    shuffle=True).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOT: predict some character completions using our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from itertools import compress\n",
    "from LSTMPostProc import LSTM_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = load_model(filepath='./models/keras/bilstm_fcnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csvpath = Path.joinpath(Path(os.getcwd()),'data/dnaseg/csv','DnaSeg4ValCharLabel.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(csvpath, delimiter=';', encoding='utf8')\n",
    "oobsents = df.seq.tolist()\n",
    "oobchars = df.label.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = sorted(list(set(oobsents[0])))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = LSTM_pred(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict one character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "indx = random.randint(0,len(oobsents))\n",
    "instr, y_true =  oobsents[indx], oobchars[indx]\n",
    "\n",
    "res = pred.predict_completion_one(instr, SEQUENCE_LENGTH, \n",
    "                                  chars, char_indices, indices_char)\n",
    "print(\"TRUE: {}| PREDICTED: {}\".format(y_true,res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple predictions: Run our model over bunch of iput data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "indx   = np.random.choice(np.arange(len(oobsents)), size=30000)\n",
    "instr  = np.array(oobsents)[indx]\n",
    "y_true = np.array(oobchars)[indx]\n",
    "\n",
    "y_pred = []\n",
    "for i in tqdm_notebook(range(len(instr))):\n",
    "    res = pred.predict_completion_one(instr[i], SEQUENCE_LENGTH, \n",
    "                                      chars, char_indices, indices_char)\n",
    "    y_pred.append(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, current_process, Queue\n",
    "\n",
    "NUM_GPUS = 1\n",
    "PROC_PER_GPU = 2    \n",
    "\n",
    "queue = Queue()\n",
    "\n",
    "def make_predict(pred,):\n",
    "    gpu_id = queue.get()\n",
    "    try:\n",
    "        # run processing on GPU <gpu_id>\n",
    "        ident = current_process().ident\n",
    "        print('{}: starting process on GPU {}'.format(ident, gpu_id))\n",
    "        res = pred.predict_completion_one(instr, SEQUENCE_LENGTH, \n",
    "                                          chars, char_indices, indices_char)\n",
    "        print('{}: finished'.format(ident))\n",
    "    finally:\n",
    "        queue.put(gpu_id)\n",
    "    return res\n",
    "\n",
    "# initialize the queue with the GPU ids\n",
    "for gpu_ids in range(NUM_GPUS):\n",
    "    for _ in range(PROC_PER_GPU):\n",
    "        queue.put(gpu_ids)\n",
    "\n",
    "pool = Pool(processes=PROC_PER_GPU * NUM_GPUS)\n",
    "batches = ['file{}.xyz'.format(x) for x in range(1000)]\n",
    "res=[]\n",
    "for out in pool.imap_unordered(make_predict, batches):\n",
    "    res.append(out)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def confusion_matrix_heatmap(y_true, y_pred):\n",
    "    '''\n",
    "    Построение Confusion matrix (матрицы ошибок)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_test: pandas.Series, numpy.array\n",
    "        Целевая для обучающего набора\n",
    "    y_pred: pandas.Series, numpy.array\n",
    "        Значения целевой переменной, предсказанные классификатором\n",
    "    '''\n",
    "    rcParams['figure.figsize'] = 6, 4\n",
    "    sns.heatmap(confusion_matrix(y_true, y_pred), annot=True, fmt='d', cmap='Greens')\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "confusion_matrix_heatmap(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call `predict_completions` instance method that wraps everything and allow us to predict multiple completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#repeatedly preparing input, asking our model for predictions and sampling from them!!!\n",
    "pred.predict_completions(instr, \n",
    "                         SEQUENCE_LENGTH, \n",
    "                         chars, \n",
    "                         char_indices, \n",
    "                         indices_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.3 (GPUAI)",
   "language": "python",
   "name": "python36_gpuai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
